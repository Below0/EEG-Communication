{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv('emotions.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of       # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  \\\n",
       "0          4.620      30.3    -356.0     15.60      26.3       1.070   \n",
       "1         28.800      33.1      32.0     25.80      22.8       6.550   \n",
       "2          8.900      29.4    -416.0     16.70      23.7      79.900   \n",
       "3         14.900      31.6    -143.0     19.80      24.3      -0.584   \n",
       "4         28.300      31.3      45.2     27.30      24.5      34.800   \n",
       "...          ...       ...       ...       ...       ...         ...   \n",
       "2127      32.400      32.2      32.2     30.80      23.4       1.640   \n",
       "2128      16.300      31.3    -284.0     14.30      23.9       4.200   \n",
       "2129      -0.547      28.3    -259.0     15.80      26.7       9.080   \n",
       "2130      16.800      19.9    -288.0      8.34      26.0       2.460   \n",
       "2131      27.000      32.0      31.8     25.00      28.9       4.990   \n",
       "\n",
       "      mean_d_1_a  mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  \\\n",
       "0          0.411     -15.700       2.060        3.15  ...      23.50   \n",
       "1          1.680       2.880       3.830       -4.82  ...     -23.30   \n",
       "2          3.360      90.200      89.900        2.03  ...     462.00   \n",
       "3         -0.284       8.820       2.300       -1.97  ...     299.00   \n",
       "4         -5.790       3.060      41.400        5.52  ...      12.00   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2127      -2.030       0.647      -0.121       -1.10  ...     -21.70   \n",
       "2128       1.090       4.460       4.720        6.63  ...     594.00   \n",
       "2129       6.900      12.700       2.030        4.64  ...     370.00   \n",
       "2130       1.580     -16.000       1.690        4.74  ...     124.00   \n",
       "2131       1.950       6.210       3.490       -3.51  ...       1.95   \n",
       "\n",
       "      fft_742_b  fft_743_b  fft_744_b  fft_745_b  fft_746_b  fft_747_b  \\\n",
       "0        20.300     20.300      23.50     -215.0     280.00    -162.00   \n",
       "1       -21.800    -21.800     -23.30      182.0       2.57     -31.60   \n",
       "2      -233.000   -233.000     462.00     -267.0     281.00    -148.00   \n",
       "3      -243.000   -243.000     299.00      132.0     -12.40       9.53   \n",
       "4        38.100     38.100      12.00      119.0     -17.60      23.90   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2127      0.218      0.218     -21.70       95.2     -19.90      47.20   \n",
       "2128   -324.000   -324.000     594.00      -35.5     142.00     -59.80   \n",
       "2129   -160.000   -160.000     370.00      408.0    -169.00     -10.50   \n",
       "2130    -27.600    -27.600     124.00     -656.0     552.00    -271.00   \n",
       "2131      1.810      1.810       1.95      110.0      -6.71      22.80   \n",
       "\n",
       "      fft_748_b  fft_749_b     label  \n",
       "0       -162.00     280.00  NEGATIVE  \n",
       "1        -31.60       2.57   NEUTRAL  \n",
       "2       -148.00     281.00  POSITIVE  \n",
       "3          9.53     -12.40  POSITIVE  \n",
       "4         23.90     -17.60   NEUTRAL  \n",
       "...         ...        ...       ...  \n",
       "2127      47.20     -19.90   NEUTRAL  \n",
       "2128     -59.80     142.00  POSITIVE  \n",
       "2129     -10.50    -169.00  NEGATIVE  \n",
       "2130    -271.00     552.00  NEGATIVE  \n",
       "2131      22.80      -6.71   NEUTRAL  \n",
       "\n",
       "[2132 rows x 2549 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data contains variables: fft(Fast fourier Tranform), correlate, entropy, logm, eigen, covmat, min_q, max_q\n",
    "# moments, mean, stddev\n",
    "# All are numerical type variable (float)\n",
    "#output is Label : Postive/Negative/Neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Class count')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#class distribution from column label (Output is label)\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(x=train_df.label, color='red')\n",
    "plt.title('Brain Wave Data', fontsize=14)\n",
    "plt.xlabel('Class Label', fontsize=14)\n",
    "plt.ylabel('Class count', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# mean_0_a    0\n",
       "mean_1_a      0\n",
       "mean_2_a      0\n",
       "mean_3_a      0\n",
       "mean_4_a      0\n",
       "             ..\n",
       "fft_746_b     0\n",
       "fft_747_b     0\n",
       "fft_748_b     0\n",
       "fft_749_b     0\n",
       "label         0\n",
       "Length: 2549, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null data\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_740_b</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>74.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>130.0</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-534.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2548 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_740_b  fft_741_b  fft_742_b  \\\n",
       "0      -15.70        2.06        3.15  ...       74.3       23.5       20.3   \n",
       "1        2.88        3.83       -4.82  ...      130.0      -23.3      -21.8   \n",
       "2       90.20       89.90        2.03  ...     -534.0      462.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...     -183.0      299.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...      114.0       12.0       38.1   \n",
       "\n",
       "   fft_743_b  fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  \n",
       "0       20.3       23.5     -215.0     280.00    -162.00    -162.00     280.00  \n",
       "1      -21.8      -23.3      182.0       2.57     -31.60     -31.60       2.57  \n",
       "2     -233.0      462.0     -267.0     281.00    -148.00    -148.00     281.00  \n",
       "3     -243.0      299.0      132.0     -12.40       9.53       9.53     -12.40  \n",
       "4       38.1       12.0      119.0     -17.60      23.90      23.90     -17.60  \n",
       "\n",
       "[5 rows x 2548 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df=train_df['label']\n",
    "train_df.drop('label', axis=1, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using cross validation (10 fold in this case)\n",
    "#Pipeline based approach\n",
    "#No of dimensions are high. Hence we will start with random forest classifier which works well on high-dimension data\n",
    "#Since its probablity based classifier, no pre-processing stages like scaling or noise removal are required\n",
    "#not affected by scale factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest =  0.9845109932602064\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "\n",
    "model_randomForest=Pipeline(steps=[('random_forest', RandomForestClassifier())])\n",
    "scores=cross_val_score(model_randomForest, train_df, label_df, cv=10, scoring='accuracy')\n",
    "print('Accuracy for Random Forest = ', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy is good and total time taken is short (4.34 secs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "\n",
    "model_logisticRegression=Pipeline(steps=[('scalar', StandardScaler()),\n",
    "                                         ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\n",
    "scores=cross_val_score(model_logisticRegression, train_df, label_df, cv=10, scoring='accuracy')\n",
    "print('Accuracy for Logistic Regression= ', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy less than Random Forest Classifier and time taken is higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaled_df=scaler.fit_transform(train_df)\n",
    "pca=PCA(n_components=20)\n",
    "pca_vectors=pca.fit_transform(scaled_df)\n",
    "for index, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(\"Explained variance ratio by Principal Component \", (index+1) ,\" : \" , var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using mathematical mapping 2549 variables mapped to 20 variables\n",
    "#Of 2549 variables, 10 are of most importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,8))\n",
    "sns.scatterplot(x=pca_vectors[:,0], y=pca_vectors[:,1],\n",
    "               hue=label_df)\n",
    "plt.title('PC V/s Class', fontsize=14)\n",
    "plt.xlabel('PC 1', fontsize=14)\n",
    "plt.ylabel('PC 2', fontsize=14)\n",
    "plt.xticks(rotation='vertical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it can be seen that if we use Logistic regression the first classifier will seperate NEUTRAL class from other two\n",
    "# and the second classifier will seperate NEGATIVE and POSITIVE\n",
    "# Applying Logistic regression model on 2 main PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "model_lg_pca=Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                            ('pca', PCA(n_components=2)),\n",
    "                            ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga',max_iter=200 ))])\n",
    "scores=cross_val_score(model_lg_pca, train_df, label_df, cv=10, scoring='accuracy')\n",
    "print('Accuracy for Logistic Regression :', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy reduced but time improved sigificantly for Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 10 PCs and running the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "model_lg_pca_10=Pipeline(steps=[('Scaler', StandardScaler()),\n",
    "                               ('pca', PCA(n_components=10)),\n",
    "                               ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\n",
    "\n",
    "scores=cross_val_score(model_lg_pca_10, train_df, label_df, cv=10, scoring='accuracy')\n",
    "print('Accuracy for Logistic Regressionwith 10 PCs :', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Accuracy of 86% compared to 2 PC cases with marginal increase in time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifical Neural Network Classifier (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "model_mlp=Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                         ('mlp_classifier', MLPClassifier(hidden_layer_sizes=(1275, 637)))])\n",
    "scores=cross_val_score(model_mlp, train_df, label_df, cv=10, scoring='accuracy')\n",
    "print('Accuracy for ANN Classifier: ', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy is good (97%) but time taken increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General convention is to start with 50% of the data size for the first hidden layer \n",
    "# and 50% of previous size in subsequent layer \n",
    "# Number of hidden layers can be taken as a hyper-parameter and can be used to tune for better accuracy\n",
    "# Hidden layers in this ccase is 2\n",
    "# Or number of hidden neurons =  average of the input and output layers summed together.\n",
    "# The upper bound on the number of hidden neurons that won't result in over-fitting is: 𝑁ℎ=𝑁𝑠/(𝛼∗(𝑁𝑖+𝑁𝑜))\n",
    "# 𝑁𝑖= number of input neurons.\n",
    "# 𝑁𝑜= number of output neurons.\n",
    "# 𝑁𝑠= number of samples in training data set.\n",
    "# α= an arbitrary scaling factor usually 2-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machines Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "model_SVM=Pipeline(steps=[('Scaler', StandardScaler()),\n",
    "                         ('svm', LinearSVC())])\n",
    "scores=cross_val_score(model_SVM, train_df, label_df, cv=10, scoring='accuracy')\n",
    "print('Accuracy for Linear SVM :', scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy little less than Random Forest Classifier but more time efficient than ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting Classifier (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import xgboost as xgb\n",
    "\n",
    "model_xgb=Pipeline(steps=\n",
    "                   [('xgboost', xgb.XGBClassifier(objective='multi:softmax'))])\n",
    "scores=cross_val_score(model_xgb, train_df, label_df, cv=10, scoring='accuracy')\n",
    "print('Accuracy for Extreme Gradient Boosting is :', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy is maximum for XGBoost but time taken is quite high.\n",
    "high running time due to internal ensemble model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost performs well in GPU Machines\n",
    "# os has been imported due to dead kernel problem\n",
    "# CONCLUSIONS\n",
    "# 1. For Accuracy XGBoost is most favourable\n",
    "# 2. Random FOrest is a perfect choice if \"time taken\" is also considered\n",
    "# 3. Simple classifiers like Logistic regression can give better accuracy with poper feauture engineering\n",
    "# 4. other classifiers don't need much feauture engineering effort"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
